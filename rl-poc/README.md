# Reinforcement Learning POC - ORAMIND

Proof of Concept (POC) progressiva de Reinforcement Learning para otimiza√ß√£o de estrat√©gias no projeto ORAMIND.

## Sobre

Este projeto cont√©m **3 n√≠veis progressivos** de implementa√ß√£o de RL, do mais simples ao mais complexo, permitindo escolher a abordagem adequada baseada na maturidade dos dados e complexidade do problema.

**Data de cria√ß√£o:** 12/12/2024
**Projeto:** ORAMIND
**Objetivo:** Otimiza√ß√£o de estrat√©gias usando aprendizado por refor√ßo

## Arquivos

### üìò Guia de Decis√£o
- **`guia_decisao_rl.py`** (16KB) - Compara√ß√£o detalhada das 3 abordagens
  - Tabela comparativa de complexidade, tempo, dados necess√°rios
  - √Årvore de decis√£o para escolher a melhor abordagem
  - Recomenda√ß√µes baseadas em cen√°rios reais
  - Calculadora de viabilidade

### üé∞ N√≠vel 1: Multi-Armed Bandit
- **`rl_poc_nivel1_bandit.py`** (12KB)
  - **Complexidade:** ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ (Muito F√°cil)
  - **Tempo de desenvolvimento:** 2-4 horas
  - **Dados necess√°rios:** 100-500 tentativas
  - **Quando usar:** Teste A/B de estrat√©gias simples
  - **Limita√ß√µes:** N√£o personaliza, n√£o otimiza sequ√™ncias

### üéØ N√≠vel 2: Contextual Bandit
- **`rl_poc_nivel2_contextual.py`** (20KB)
  - **Complexidade:** ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (M√©dio)
  - **Tempo de desenvolvimento:** 1-2 semanas
  - **Dados necess√°rios:** 1.000-5.000 tentativas
  - **Quando usar:** Personaliza√ß√£o por perfil de cliente
  - **Vantagens:** Personaliza a√ß√µes por contexto
  - **Limita√ß√µes:** N√£o otimiza sequ√™ncias longas

### üß† N√≠vel 3: Q-Learning
- **`rl_poc_nivel3_qlearning.py`** (23KB)
  - **Complexidade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Dif√≠cil)
  - **Tempo de desenvolvimento:** 3-4 semanas
  - **Dados necess√°rios:** 5.000-20.000 tentativas
  - **Quando usar:** Otimiza√ß√£o de jornadas completas
  - **Vantagens:** Otimiza sequ√™ncias, personaliza√ß√£o avan√ßada
  - **Desafios:** Alta complexidade, requer muito dado

## Compara√ß√£o R√°pida

| Crit√©rio | N√≠vel 1 (Bandit) | N√≠vel 2 (Contextual) | N√≠vel 3 (Q-Learning) |
|----------|------------------|----------------------|----------------------|
| **Implementa√ß√£o** | 2-4 horas | 1-2 semanas | 3-4 semanas |
| **Dados M√≠nimos** | 100-500 | 1.000-5.000 | 5.000-20.000 |
| **Personaliza√ß√£o** | ‚ùå N√£o | ‚úÖ Por perfil | ‚úÖ‚úÖ Avan√ßada |
| **Otimiza√ß√£o Sequencial** | ‚ùå N√£o | ‚ùå N√£o | ‚úÖ Sim |
| **Explicabilidade** | ‚úÖ‚úÖ‚úÖ Alta | ‚úÖ‚úÖ M√©dia | ‚ö†Ô∏è Baixa |
| **Manuten√ß√£o** | ‚úÖ‚úÖ‚úÖ F√°cil | ‚úÖ‚úÖ M√©dia | ‚ö†Ô∏è Complexa |

## Como Usar

### 1. Avalie seu cen√°rio
```bash
python3 guia_decisao_rl.py
```

O guia ir√°:
- Comparar as 3 abordagens em tabela detalhada
- Fornecer √°rvore de decis√£o
- Sugerir a melhor abordagem para seu caso
- Calcular viabilidade baseada em suas respostas

### 2. Execute a POC escolhida

#### N√≠vel 1 - Multi-Armed Bandit
```bash
python3 rl_poc_nivel1_bandit.py
```
**Use quando:**
- Precisa de resultado r√°pido (2-4 horas)
- Tem poucos dados (100-500 tentativas)
- Quer testar A/B de estrat√©gias
- N√£o precisa de personaliza√ß√£o

#### N√≠vel 2 - Contextual Bandit
```bash
python3 rl_poc_nivel2_contextual.py
```
**Use quando:**
- Tem dados de contexto (perfil, hist√≥rico)
- Quer personalizar por tipo de cliente
- Tem 1.000-5.000 tentativas
- Tempo de 1-2 semanas dispon√≠vel

#### N√≠vel 3 - Q-Learning
```bash
python3 rl_poc_nivel3_qlearning.py
```
**Use quando:**
- Precisa otimizar jornadas completas
- Tem muitos dados (5.000-20.000 tentativas)
- Timeline de 3-4 semanas
- Precisa de personaliza√ß√£o avan√ßada

## Estrutura dos Scripts

Todos os scripts seguem a mesma estrutura:

```python
# 1. Configura√ß√£o e Imports
# 2. Gera√ß√£o de Dados Sint√©ticos (para demonstra√ß√£o)
# 3. Implementa√ß√£o do Algoritmo RL
# 4. Treinamento e Otimiza√ß√£o
# 5. Visualiza√ß√£o de Resultados
# 6. An√°lise de Performance
```

## Depend√™ncias

```bash
pip install numpy pandas matplotlib scikit-learn
```

**Vers√µes testadas:**
- Python 3.8+
- NumPy 1.21+
- Pandas 1.3+
- Matplotlib 3.4+
- Scikit-learn 1.0+

## Resultados Esperados

Cada script gera:
- **Gr√°ficos de converg√™ncia** - Como o algoritmo aprende ao longo do tempo
- **M√©tricas de performance** - Taxa de sucesso, recompensa acumulada
- **An√°lise comparativa** - Performance vs baseline
- **Recomenda√ß√µes** - Melhores a√ß√µes por contexto (n√≠vel 2 e 3)

## Projeto ORAMIND

### Contexto
ORAMIND √© um sistema de otimiza√ß√£o de estrat√©gias que precisa:
- Aprender as melhores a√ß√µes baseado em feedback
- Personalizar recomenda√ß√µes por perfil
- Otimizar jornadas completas do usu√°rio
- Balancear explora√ß√£o vs explora√ß√£o

### Aplica√ß√µes
- **N√≠vel 1:** Teste A/B de mensagens/ofertas
- **N√≠vel 2:** Personaliza√ß√£o de estrat√©gias por segmento
- **N√≠vel 3:** Otimiza√ß√£o de jornadas multicanal

## √Årvore de Decis√£o Simplificada

```
Tem dados de sequ√™ncias/jornadas?
‚îú‚îÄ‚îÄ SIM: N√≠vel 3 (Q-Learning)
‚îî‚îÄ‚îÄ N√ÉO: Tem dados de contexto/perfil?
    ‚îú‚îÄ‚îÄ SIM: N√≠vel 2 (Contextual Bandit)
    ‚îî‚îÄ‚îÄ N√ÉO: N√≠vel 1 (Multi-Armed Bandit)
```

## Pr√≥ximos Passos

1. **Avalia√ß√£o:** Execute `guia_decisao_rl.py` para escolher abordagem
2. **Prototipagem:** Teste com dados sint√©ticos
3. **Valida√ß√£o:** Adapte para dados reais do ORAMIND
4. **Deploy:** Integre com sistema de produ√ß√£o
5. **Monitoramento:** Acompanhe performance e ajuste

## Limita√ß√µes

### N√≠vel 1 (Bandit)
- ‚ùå N√£o considera contexto do usu√°rio
- ‚ùå N√£o otimiza sequ√™ncias
- ‚úÖ R√°pido e f√°cil de implementar

### N√≠vel 2 (Contextual)
- ‚ùå N√£o otimiza sequ√™ncias longas
- ‚ö†Ô∏è Requer engenharia de features
- ‚úÖ Personaliza por perfil

### N√≠vel 3 (Q-Learning)
- ‚ùå Complexo de debugar
- ‚ùå Requer muito dado
- ‚ö†Ô∏è Pode demorar para convergir
- ‚úÖ Otimiza√ß√£o completa de jornadas

## Refer√™ncias

- **Multi-Armed Bandit:** Sutton & Barto (2018) - Reinforcement Learning: An Introduction
- **Contextual Bandit:** Li et al. (2010) - Contextual Bandits Approach
- **Q-Learning:** Watkins & Dayan (1992) - Q-Learning Algorithm

## Manuten√ß√£o

**Criado em:** 12/12/2024
**√öltima atualiza√ß√£o:** 06/01/2026
**Mantido por:** Marcelo
**Status:** ‚úÖ POC Completa

---

**D√∫vidas?** Consulte o `guia_decisao_rl.py` para escolher a melhor abordagem!
